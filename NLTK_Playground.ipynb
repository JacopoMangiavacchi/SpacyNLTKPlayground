{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK Cheat Sheet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHWSCQ_37Pn-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "22b8fe00-4ffe-4e08-95dc-a12fb415aa4b"
      },
      "source": [
        "import nltk\n",
        " \n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnBnCJ8R-PM_",
        "colab_type": "text"
      },
      "source": [
        "**TOKEN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce64AB6C7bJp",
        "colab_type": "code",
        "outputId": "6be4416a-2138-4ba3-830b-972a66ed2c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        " \n",
        "doc = \"Apples and oranges are similar. Boots and hippos aren't.\"\n",
        "print(word_tokenize(doc))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Apples', 'and', 'oranges', 'are', 'similar', '.', 'Boots', 'and', 'hippos', 'are', \"n't\", '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZtEf9mb-LvF",
        "colab_type": "text"
      },
      "source": [
        "**SPAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsQgsaZm7jV4",
        "colab_type": "code",
        "outputId": "d49de9fd-a0cb-4976-df69-460e21003a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer as twt\n",
        "list(twt().span_tokenize('What is the airspeed of an unladen swallow ?'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 4),\n",
              " (5, 7),\n",
              " (8, 11),\n",
              " (12, 20),\n",
              " (21, 23),\n",
              " (24, 26),\n",
              " (27, 34),\n",
              " (35, 42),\n",
              " (43, 44)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omICr1wOI60i",
        "colab_type": "text"
      },
      "source": [
        "**STOP WORDS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmCx5pbkI8Gi",
        "colab_type": "code",
        "outputId": "3baa9c07-d527-4d9e-8b90-6318d43f4a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopWords = set(stopwords.words('english'))\n",
        "\n",
        "print('Number of stop words: %d' % len(stopWords))\n",
        "print('First ten stop words: %s' % list(stopWords)[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of stop words: 179\n",
            "First ten stop words: ['when', \"mightn't\", 'it', 'do', 'am', 'during', 'her', 'because', \"don't\", 'are']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-7VlZkviPTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7d5897d1-9231-4461-a7a9-e0125bc5d639"
      },
      "source": [
        "doc = \"I live in New York City, the capital of the New York State\"\n",
        "tokens = [token for token in word_tokenize(doc) if token not in stopWords]\n",
        "\n",
        "print('Original Article: %s' % (doc))\n",
        "print()\n",
        "print(tokens)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Article: I live in New York City, the capital of the New York State\n",
            "\n",
            "['I', 'live', 'New', 'York', 'City', ',', 'capital', 'New', 'York', 'State']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUaL9Vv7JAbU",
        "colab_type": "text"
      },
      "source": [
        "**LEMMATIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3mhaO8WJBS5",
        "colab_type": "code",
        "outputId": "49518324-e850-4fe3-e610-37aec1bcc40c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "doc = \"Apples and oranges are similar. Boots and hippos aren't.\"\n",
        "tokens = word_tokenize(doc)\n",
        "print(tokens)\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(lemmas)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Apples', 'and', 'oranges', 'are', 'similar', '.', 'Boots', 'and', 'hippos', 'are', \"n't\", '.']\n",
            "['Apples', 'and', 'orange', 'are', 'similar', '.', 'Boots', 'and', 'hippo', 'are', \"n't\", '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqELOyOLnDzt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be9f6844-3200-4124-88ab-26ac8d179591"
      },
      "source": [
        "lemmatizer.lemmatize(\"are\", \"v\") #POS"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'be'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqs474kc-an_",
        "colab_type": "text"
      },
      "source": [
        "**PART-OF-SPEECH and Syntactic dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l15Fd1u78uHP",
        "colab_type": "code",
        "outputId": "e50b1729-ce3c-46ba-db7e-fc500f9a52f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "doc = \"This is a text.\"\n",
        "print(word_tokenize(doc))\n",
        "print(nltk.pos_tag(word_tokenize(doc)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'text', '.']\n",
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('text', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhqK-BAD-hyD",
        "colab_type": "text"
      },
      "source": [
        "**NAMED ENTITIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKci3dyI9jXy",
        "colab_type": "code",
        "outputId": "1ba13180-c008-4e9b-ad8b-dcd752a2eaa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "doc = \"Larry Page founded Google\"\n",
        "\n",
        "for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(doc))):\n",
        "  if hasattr(chunk, 'label'):\n",
        "     print(chunk.label(), ' '.join(c[0] for c in chunk))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PERSON Larry\n",
            "PERSON Page\n",
            "PERSON Google\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SmWNYn_-nW-",
        "colab_type": "text"
      },
      "source": [
        "**SENTENCES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-uj4ENR9-Rf",
        "colab_type": "code",
        "outputId": "b267a527-0087-4595-f405-22092d84b992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
        "print(sent_tokenize(data))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['All work and no play makes jack dull boy.', 'All work and no play makes jack a dull boy.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FBWLlvg-rol",
        "colab_type": "text"
      },
      "source": [
        "**CHUNK and Base noun phrases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSVr7xm8-pd4",
        "colab_type": "code",
        "outputId": "0a1e75db-e6b4-4f75-9f6e-346d9b040998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "doc = \"I have a red car\"\n",
        "for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(doc))):\n",
        "  print(chunk)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('I', 'PRP')\n",
            "('have', 'VBP')\n",
            "('a', 'DT')\n",
            "('red', 'JJ')\n",
            "('car', 'NN')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}